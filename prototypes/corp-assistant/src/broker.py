import io
import logging
import time
from collections.abc import Iterator

from aiogram import Bot
from aiogram.types import BufferedInputFile, Message
from faststream import FastStream, Logger
from faststream.redis import RedisBroker
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydub import AudioSegment
from pydub.utils import make_chunks

from .core import schemas
from .integrations import salute_speech
from .settings import PROMPTS_DIR, settings
from .utils import current_datetime, md_to_pdf, progress_emojis

logger = logging.getLogger(__name__)

broker = RedisBroker(
    settings.redis.url,
    socket_timeout=120.0,
    socket_connect_timeout=15.0,
    socket_keepalive=True,
    retry_on_timeout=True,
    health_check_interval=15
)

app = FastStream(broker)

MEETING_MINUTES_PROMPT = (PROMPTS_DIR / "meeting_minutes_prompt.md").read_text(encoding="utf-8")


def split_audio_into_segments(
        audio_data: bytes, audio_format: str, segment_duration_ms: int = 60 * 20 * 1000
) -> Iterator[schemas.AudioSegment]:
    """–†–∞–∑–¥–µ–ª—è–µ—Ç –∞—É–¥–∏–æ —Ñ–∞–π–ª –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã —Å –∑–∞–¥–∞–Ω–Ω–æ–π –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é.

    :param audio_data: –ë–∞–π—Ç—ã –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞.
    :param audio_format: –§–æ—Ä–º–∞—Ç –∞—É–¥–∏–æ, –Ω–∞–ø—Ä–∏–º–µ—Ä: 'wav', 'ogg', 'm4a'.
    :param segment_duration_ms: –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö.
    :returns: –û–±—ä–µ–∫—Ç—ã –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤.
    """

    logger.info("Start split audio on segments...")
    audio = AudioSegment.from_file(io.BytesIO(audio_data), format=audio_format)
    chunks = make_chunks(audio, segment_duration_ms)
    chunks_count = len(chunks)
    logger.info("Created %s segments from audio", chunks_count)
    for i, chunk in enumerate(chunks):
        buffer = io.BytesIO()
        chunk.export(buffer, format="wav", bitrate="192k")
        logger.info("Export %s segment data to WAV format", i)
        chunk_data = buffer.getvalue()
        yield schemas.AudioSegment(
            index=i,
            segments_count=chunks_count,
            data=chunk_data,
            size=len(chunk_data),
            audio_format="wav",
            duration_ms=segment_duration_ms
        )


async def update_progress(
        bot: Bot, chat_id: int, percent: float, prev_message_id: int | None = None
) -> Message:
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏ –∞—É–¥–∏–æ –∑–∞–ø–∏—Å–∏"""

    text = f"""
    –†–∞—Å—à–∏—Ñ—Ä–æ–≤—ã–≤–∞—é –∞—É–¥–∏–æ ...
    {progress_emojis(percent)}
    <b>{percent:.1f}%</b>
    """
    await bot.delete_message(chat_id=chat_id, message_id=prev_message_id)
    return await bot.send_message(chat_id=chat_id, text=text)


async def generate_meeting_minutes(transcription: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª —Å–æ–≤–µ—â–∞–Ω–∏—è –ø–æ –µ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏.

    :param transcription: –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å–æ–≤–µ—â–∞–Ω–∏—è.
    :returns: –°–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª –≤ Markdown —Ñ–æ—Ä–º–∞—Ç–µ.
    """

    model = ChatOpenAI(
        api_key=settings.yandexcloud.apikey,
        model=settings.yandexcloud.qwen3_235b,
        base_url=settings.yandexcloud.base_url,
        temperature=0.2,
        max_retries=3,
    )
    prompt = ChatPromptTemplate.from_template(MEETING_MINUTES_PROMPT)
    chain = prompt | model | StrOutputParser()
    return await chain.ainvoke({"transcription": transcription})


@broker.subscriber("minutes:draw_up")
async def process_minutes_task(task: schemas.MinutesTask, logger: Logger) -> None:
    from .bot import bot  # noqa: PLC0415

    bot_message = await bot.send_message(
        chat_id=task.user_id,
        text="–°–∫–∞—á–∏–≤–∞—é –∞—É–¥–∏–æ —Ñ–∞–π–ª üîú ..."
    )
    transcription_segments: list[str] = []
    start_time = time.time()
    file_buffer = await bot.download_file(task.audio_path, destination=io.BytesIO())
    audio_data = file_buffer.getbuffer().tobytes()
    for audio_segment in split_audio_into_segments(audio_data, audio_format=task.audio_format):
        bot_message = await update_progress(
            bot=bot,
            chat_id=task.user_id,
            percent=audio_segment.index + 1 / audio_segment.segments_count * 100,
            prev_message_id=bot_message.message_id
        )
        logger.info(
            "Recognizing %s/%s segment of audio file `%s`",
            audio_segment.index + 1, audio_segment.segments_count, task.audio_path
        )
        transcription = await salute_speech.recognize_async(
            audio_data=audio_segment.data,
            audio_encoding="PCM_S16LE",
            max_speakers=task.max_speakers,
        )
        transcription_segments.append(transcription)
    full_transcription = "\n".join(transcription_segments)
    await bot.delete_message(chat_id=task.user_id, message_id=bot_message.message_id)
    await bot.send_message(
        chat_id=task.user_id,
        text="–í—Å—ë —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ! üé§\n–§–æ—Ä–º–∏—Ä—É—é –ø—Ä–æ—Ç–æ–∫–æ–ª —Å–æ–≤–µ—â–∞–Ω–∏—è‚Ä¶ ‚úçÔ∏è\n–≠—Ç–æ –∑–∞–π–º—ë—Ç –µ—â—ë 30‚Äì90 —Å–µ–∫—É–Ω–¥",
    )
    md_content = await generate_meeting_minutes(full_transcription)
    execution_time = time.time() - start_time
    logger.info("Minutes of meeting completed, it took %s seconds", round(execution_time, 2))
    md_content = md_content.replace("```", "").replace("markdown", "")
    pdf_file = md_to_pdf(md_content)
    await bot.send_document(
        chat_id=task.user_id, document=BufferedInputFile(
            file=pdf_file,
            filename=f"–ü—Ä–æ–∫–æ–ª_—Å–æ–≤–µ—â–∞–Ω–∏—è_{current_datetime()}.pdf"
        ),
        caption="–í–∞—à –ø—Ä–æ—Ç–æ–∫–æ–ª —Å–æ–≤–µ—â–∞–Ω–∏—è –≥–æ—Ç–æ–≤! üéâ"
    )
